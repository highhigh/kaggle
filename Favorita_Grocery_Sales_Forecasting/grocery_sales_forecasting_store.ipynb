{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grocery_sales_forecasting_storewise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline model is non-parametric. This model is simply average of history sale (from 2013 to 2016 of the same day.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from time import time\n",
    "import sys\n",
    "\n",
    "types = {'id': 'int32', 'item_nbr': 'int32', 'store_nbr': 'int8', 'onpromotion': bool}\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 19 items appear in training data, but not in testing data; 68 items appear only testing data, not in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "def pre_process(feature_raw):\n",
    "\n",
    "    items = pd.read_csv(\"input/items.csv\", dtype={'perishable': np.dtype('int8')})\n",
    "    feature_raw= pd.merge(feature_raw,items, right_on='item_nbr',left_on='item_nbr',how='left')\n",
    "    \n",
    "#     oil = pd.read_csv(\"input/oil.csv\")\n",
    "#     oil.fillna(method ='bfll', inplace = True)\n",
    "#     oil['dcoilwtico'] = oil['dcoilwtico'].apply(np.log1p)\n",
    "#     feature_raw= pd.merge(feature_raw,oil, right_on='date',left_on='date',how='left')\n",
    "    \n",
    "    feature_raw['onpromotion'].fillna(False, inplace = True)\n",
    "    feature_raw['onpromotion'] = feature_raw['onpromotion'].map({False : 0, True : 1})\n",
    "    feature_raw['onpromotion'] = feature_raw['onpromotion'].astype('int8')\n",
    "\n",
    "    # holidays_events = pd.read_csv(\"input/holidays_events.csv\")\n",
    "    # train_data= pd.merge(train_data,stores, right_on='date',left_on='date',how='left')\n",
    "    # display(train_data.tail(5))\n",
    "    \n",
    "    feature_raw['day_of_week'] = feature_raw['date'].dt.weekday_name\n",
    "\n",
    "    feature_raw.drop(['date', 'store_nbr'], axis = 1, inplace = True)\n",
    "    \n",
    "    feature_one_hot_coded = pd.get_dummies(feature_raw, columns = ['item_nbr', 'family', 'class', 'day_of_week'])\n",
    "\n",
    "    if feature_one_hot_coded.isnull().values.any():\n",
    "        print(\"There is NaN after one-hot-encoding!\")\n",
    "    else:\n",
    "        print(\"There is no NaN after one-hot-encoding\")\n",
    "        \n",
    "    return feature_one_hot_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(store):\n",
    "    filename_train = 'train_store_' + str(store) + '.csv'\n",
    "    train_data_raw = pd.read_csv('input_each_store/' + filename_train, usecols = [1,2,3,4,5],\n",
    "                                 parse_dates=['date'], dtype=types, \n",
    "                                 infer_datetime_format = True,\n",
    "                                 converters={'unit_sales':lambda u: float(u) if float(u)>0 else 0},)\n",
    "    print(\"Raw training data has {} samples with {} features each.\".format(*train_data_raw.shape))\n",
    "#     display(train_data_raw.head(5))\n",
    "    \n",
    "    filename_test = 'test_store_' + str(store) + '.csv'\n",
    "    test_data_raw = pd.read_csv('input_each_store/' + filename_test, usecols=[1,2,3,4], parse_dates=['date'], dtype=types)\n",
    "    print(\"Public testing data has {} samples with {} features each.\".format(*test_data_raw.shape))\n",
    "#     display(test_data_raw.head(5))\n",
    "    \n",
    "    uitem_train = set(train_data_raw.item_nbr.unique())\n",
    "    uitem_test = set(test_data_raw.item_nbr.unique())\n",
    "    items_trained = uitem_train.intersection(uitem_test)\n",
    "    \n",
    "    train_data = train_data_raw.loc[train_data_raw['item_nbr'].isin(items_trained)].copy()\n",
    "    print(\"Training data has {} samples after droping unuseful items.\".format(train_data.shape[0]))\n",
    "    \n",
    "    unit_sales = train_data['unit_sales'].apply(np.log1p).values\n",
    "    train_data.drop('unit_sales', axis = 1, inplace=True)\n",
    "    train_processed = pre_process(train_data)\n",
    "    \n",
    "    return [train_processed, unit_sales, items_trained]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metric: Normalized Weighted Root Mean Squared Logarithmic Error (NWRMSLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def nwrmsle(ground_truth, predictions, w):\n",
    "    return metrics.mean_squared_error(ground_truth, predictions, sample_weight=w)**0.5\n",
    "# metrics.nwrmsle_scorer = make_scorer(score_func=nwrmsle, greater_is_better=False, w = [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_predict(learner, X_train, y_train, X_test, y_test):\n",
    "    results = dict()\n",
    "    start = time()\n",
    "    sample_size=X_train.shape[0]\n",
    "    learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    results['train_time'] = time() - start\n",
    "    \n",
    "    start = time()\n",
    "    predictions_temp = learner.predict(X_test)\n",
    "    predictions = [x if x>=0 else 0 for x in predictions_temp]\n",
    "    results['test_time'] = time() - start\n",
    "    results['learner'] = learner\n",
    "    \n",
    "    w = X_test['perishable'].apply(lambda x: 1.25 if x else 1).values.ravel()\n",
    "    results['score'] = nwrmsle(y_test.flatten(), predictions, w = w)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_submission(model, columns, store, items_trained):\n",
    "    filename_test = 'test_store_' + str(store) + '.csv'\n",
    "    test_data_raw = pd.read_csv('input_each_store/' + filename_test, usecols=[0,1,2,3,4], parse_dates=['date'], dtype=types)\n",
    "    \n",
    "    test_data = test_data_raw.loc[test_data_raw['item_nbr'].isin(items_trained)].copy()\n",
    "    test_data.drop('id', axis = 1, inplace=True)\n",
    "    \n",
    "    test_one_hot_encoded = pre_process(test_data)\n",
    "    test_processed = pd.DataFrame(0, index = range(test_data.shape[0]), columns=columns, dtype=np.dtype('int8'))\n",
    "    test_processed[test_one_hot_encoded.columns] = test_one_hot_encoded\n",
    "#     print(test_processed.isnull().values.any())\n",
    "    predictions = model.predict(test_processed)\n",
    "    test_data['unit_sales'] = [np.expm1(p) for p in predictions]\n",
    "    \n",
    "    test_store= pd.merge(test_data_raw, test_data, left_on=['date','item_nbr', 'onpromotion'],\n",
    "                      right_on=['date','item_nbr', 'onpromotion'],how='left')[['id', 'unit_sales']]\n",
    "    test_store.fillna(0, inplace=True)\n",
    "    return test_store.set_index(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Store 1:\n",
      "Raw training data has 1702008 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1639533 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1639533 samples with 3717 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 2:\n",
      "Raw training data has 1991925 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1919599 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1919599 samples with 3743 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 3:\n",
      "Raw training data has 2289721 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2201865 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2201865 samples with 3811 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 4:\n",
      "Raw training data has 1883148 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1814281 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1814281 samples with 3690 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 5:\n",
      "Raw training data has 1744950 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1680637 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1680637 samples with 3672 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 6:\n",
      "Raw training data has 2066076 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1991765 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1991765 samples with 3739 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 7:\n",
      "Raw training data has 1955032 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1879672 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1879672 samples with 3779 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 8:\n",
      "Raw training data has 2187898 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2104517 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2104517 samples with 3818 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 9:\n",
      "Raw training data has 1863587 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1797636 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1797636 samples with 3365 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 10:\n",
      "Raw training data has 1144041 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1108701 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1108701 samples with 2768 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 11:\n",
      "Raw training data has 1729115 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1671188 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1671188 samples with 3308 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 12:\n",
      "Raw training data has 1225352 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1186905 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1186905 samples with 2805 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 13:\n",
      "Raw training data has 1159697 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1121659 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1121659 samples with 2688 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 14:\n",
      "Raw training data has 1233149 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1197366 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1197366 samples with 2747 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 15:\n",
      "Raw training data has 1304980 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1264030 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1264030 samples with 2832 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 16:\n",
      "Raw training data has 1181683 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1143517 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1143517 samples with 2803 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 17:\n",
      "Raw training data has 1484900 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1437732 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1437732 samples with 2928 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 18:\n",
      "Raw training data has 1404158 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1356814 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1356814 samples with 3377 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 19:\n",
      "Raw training data has 1269669 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1228722 samples after droping unuseful items.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1228722 samples with 2783 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 20:\n",
      "Raw training data has 1665985 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1607781 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1607781 samples with 3333 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 21:\n",
      "Raw training data has 1373419 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1326405 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1326405 samples with 3300 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 22:\n",
      "Raw training data has 923483 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 895107 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 895107 samples with 2828 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 23:\n",
      "Raw training data has 1556095 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1502970 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1502970 samples with 3621 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 24:\n",
      "Raw training data has 1946961 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1871877 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1871877 samples with 3740 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 25:\n",
      "Raw training data has 1336493 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1284163 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1284163 samples with 3664 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 26:\n",
      "Raw training data has 1145852 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1104697 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1104697 samples with 3588 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 27:\n",
      "Raw training data has 1773923 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1704963 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1704963 samples with 3743 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 28:\n",
      "Raw training data has 1625989 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1566827 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1566827 samples with 3272 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 29:\n",
      "Raw training data has 1508959 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1452592 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1452592 samples with 3265 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 30:\n",
      "Raw training data has 1069327 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1031655 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1031655 samples with 2786 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 31:\n",
      "Raw training data has 1590703 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1535667 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1535667 samples with 3260 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 32:\n",
      "Raw training data has 895389 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 865204 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 865204 samples with 2688 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 33:\n",
      "Raw training data has 1303800 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1262763 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1262763 samples with 2802 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 34:\n",
      "Raw training data has 1517167 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1462497 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1462497 samples with 3272 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 35:\n",
      "Raw training data has 927581 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 897841 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 897841 samples with 2741 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 36:\n",
      "Raw training data has 1569397 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1512375 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1512375 samples with 3342 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 37:\n",
      "Raw training data has 1882975 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1811197 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1811197 samples with 3735 variables.\n",
      "There is no NaN before one-hot-encoding\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 38:\n",
      "Raw training data has 1808298 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1740349 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1740349 samples with 3675 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 39:\n",
      "Raw training data has 1774406 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1711651 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1711651 samples with 3387 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 40:\n",
      "Raw training data has 1436643 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1393792 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1393792 samples with 2845 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 41:\n",
      "Raw training data has 1700624 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1638204 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1638204 samples with 3695 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 42:\n",
      "Raw training data has 1461683 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1405391 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1405391 samples with 3742 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 43:\n",
      "Raw training data has 1342425 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1300670 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1300670 samples with 3279 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 44:\n",
      "Raw training data has 2364891 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2274095 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2274095 samples with 3834 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 45:\n",
      "Raw training data has 2349153 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2260136 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2260136 samples with 3838 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 46:\n",
      "Raw training data has 2265734 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2182782 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2182782 samples with 3807 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 47:\n",
      "Raw training data has 2333689 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2246178 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2246178 samples with 3842 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 48:\n",
      "Raw training data has 2172293 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2093020 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2093020 samples with 3795 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 49:\n",
      "Raw training data has 2275386 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2188795 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2188795 samples with 3825 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 50:\n",
      "Raw training data has 2145338 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 2067217 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 2067217 samples with 3814 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 51:\n",
      "Raw training data has 1976071 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1899781 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1899781 samples with 3802 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 52:\n",
      "Raw training data has 290581 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 288642 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 288642 samples with 3758 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 53:\n",
      "Raw training data has 1664077 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1599174 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1599174 samples with 3675 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "------------------------------\n",
      "Store 54:\n",
      "Raw training data has 1106897 samples with 5 features each.\n",
      "Public testing data has 62416 samples with 4 features each.\n",
      "Training data has 1072684 samples after droping unuseful items.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n",
      "Processed training set has 1072684 samples with 2720 variables.\n",
      "There is no NaN before one-hot-encoding\n",
      "There is no NaN after one-hot-encoding\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "results = {}\n",
    "store_list = range(1,55)\n",
    "predictions = pd.DataFrame(0, index = range(125497040, 128867504), columns = ['unit_sales'])\n",
    "predictions.index.name = 'id'\n",
    "\n",
    "regressor_A = DecisionTreeRegressor(random_state = 36, max_depth = 50)\n",
    "regressor_B = linear_model.LinearRegression()\n",
    "params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split':2, 'learning_rate': 0.01}\n",
    "regressor_C = GradientBoostingRegressor(**params)\n",
    "\n",
    "regressor = regressor_A\n",
    "reg_name = regressor.__class__.__name__\n",
    "\n",
    "file_submission_temp = reg_name + dt.datetime.today().strftime(\"%Y-%m-%d\") + '_temp.csv'\n",
    "file_submission = reg_name + dt.datetime.today().strftime(\"%Y-%m-%d\") + '.csv'\n",
    "\n",
    "for store in store_list:\n",
    "    print(\"------------------------------\")\n",
    "    print(\"Store {}:\".format(store))\n",
    "    train_processed, unit_sales, items_trained = load_data(store)\n",
    "    print(\"Processed training set has {} samples with {} variables.\".format(*train_processed.shape))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(train_processed, unit_sales, test_size = 0.1, random_state = 42)\n",
    "\n",
    "    results[store]= train_predict(regressor, X_train, y_train, X_test, y_test)\n",
    "    prediction_store = predict_submission(results[store]['learner'], train_processed.columns.values, store, items_trained)\n",
    "    \n",
    "    predictions.loc[prediction_store.index] = prediction_store \n",
    "    if not os.path.isfile(file_submission_temp):\n",
    "        prediction_store.to_csv(file_submission_temp, float_format='%.3f')\n",
    "    else:\n",
    "        prediction_store.to_csv(file_submission_temp, mode = 'a', header = False, float_format='%.3f')\n",
    "\n",
    "\n",
    "predictions.to_csv(file_submission, float_format='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.51493135429956427,\n",
       "  'test_time': 0.8361241817474365,\n",
       "  'train_time': 494.2959461212158},\n",
       " 2: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.54035929277014993,\n",
       "  'test_time': 0.983651876449585,\n",
       "  'train_time': 461.6026873588562},\n",
       " 3: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.60726924538986271,\n",
       "  'test_time': 3.0506207942962646,\n",
       "  'train_time': 640.3319079875946},\n",
       " 4: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.53114905298255555,\n",
       "  'test_time': 0.9380218982696533,\n",
       "  'train_time': 478.5305509567261},\n",
       " 5: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.57834145677944926,\n",
       "  'test_time': 0.8324646949768066,\n",
       "  'train_time': 1129.899358510971},\n",
       " 6: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.56149236638556055,\n",
       "  'test_time': 1.0242867469787598,\n",
       "  'train_time': 521.400553226471},\n",
       " 7: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.56272102224035869,\n",
       "  'test_time': 0.9700045585632324,\n",
       "  'train_time': 449.84612345695496},\n",
       " 8: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.55768741358968044,\n",
       "  'test_time': 1.8401780128479004,\n",
       "  'train_time': 540.1976869106293},\n",
       " 9: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.64819778527878313,\n",
       "  'test_time': 0.7955508232116699,\n",
       "  'train_time': 1020.6157603263855},\n",
       " 10: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.58043287086613582,\n",
       "  'test_time': 0.4347414970397949,\n",
       "  'train_time': 643.5472495555878},\n",
       " 11: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.6773163019857108,\n",
       "  'test_time': 0.7754561901092529,\n",
       "  'train_time': 763.6096765995026},\n",
       " 12: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.58962376348111434,\n",
       "  'test_time': 0.4655930995941162,\n",
       "  'train_time': 530.4363086223602},\n",
       " 13: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.59974123602473084,\n",
       "  'test_time': 0.4300382137298584,\n",
       "  'train_time': 482.101726770401},\n",
       " 14: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.61393832813264748,\n",
       "  'test_time': 0.4582369327545166,\n",
       "  'train_time': 687.9462766647339},\n",
       " 15: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.57005758229874381,\n",
       "  'test_time': 0.4994242191314697,\n",
       "  'train_time': 605.8803989887238},\n",
       " 16: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.58222280371747126,\n",
       "  'test_time': 0.43752074241638184,\n",
       "  'train_time': 604.3780469894409},\n",
       " 17: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.58075750811717619,\n",
       "  'test_time': 0.6003994941711426,\n",
       "  'train_time': 297.43981647491455},\n",
       " 18: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.53595484265065751,\n",
       "  'test_time': 0.6386547088623047,\n",
       "  'train_time': 280.0064299106598},\n",
       " 19: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.58473732639355902,\n",
       "  'test_time': 0.4826178550720215,\n",
       "  'train_time': 482.74173974990845},\n",
       " 20: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.60787677250847771,\n",
       "  'test_time': 0.742027997970581,\n",
       "  'train_time': 699.1862528324127},\n",
       " 21: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.61250263618834089,\n",
       "  'test_time': 0.6026179790496826,\n",
       "  'train_time': 807.7122292518616},\n",
       " 22: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.54851771868212351,\n",
       "  'test_time': 0.35636377334594727,\n",
       "  'train_time': 427.4596118927002},\n",
       " 23: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.52027675576152432,\n",
       "  'test_time': 0.7627723217010498,\n",
       "  'train_time': 784.0233850479126},\n",
       " 24: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.56026289393323392,\n",
       "  'test_time': 0.9424839019775391,\n",
       "  'train_time': 454.07847476005554},\n",
       " 25: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.54950884946367129,\n",
       "  'test_time': 0.64308762550354,\n",
       "  'train_time': 318.53503918647766},\n",
       " 26: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.53228201338528502,\n",
       "  'test_time': 0.5407772064208984,\n",
       "  'train_time': 778.1209366321564},\n",
       " 27: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.53768930211269073,\n",
       "  'test_time': 0.8762636184692383,\n",
       "  'train_time': 366.54380536079407},\n",
       " 28: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.62102246681417883,\n",
       "  'test_time': 0.7381749153137207,\n",
       "  'train_time': 880.8138444423676},\n",
       " 29: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.56572266249479697,\n",
       "  'test_time': 0.6345582008361816,\n",
       "  'train_time': 412.28140234947205},\n",
       " 30: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.51598412137790084,\n",
       "  'test_time': 0.40569329261779785,\n",
       "  'train_time': 374.0105459690094},\n",
       " 31: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.64718854249922331,\n",
       "  'test_time': 0.6957418918609619,\n",
       "  'train_time': 948.1439807415009},\n",
       " 32: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.54336490007593707,\n",
       "  'test_time': 0.32440733909606934,\n",
       "  'train_time': 504.3122732639313},\n",
       " 33: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.6249803273825818,\n",
       "  'test_time': 0.49907946586608887,\n",
       "  'train_time': 667.2293272018433},\n",
       " 34: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.58034554118593162,\n",
       "  'test_time': 0.6604499816894531,\n",
       "  'train_time': 468.9371728897095},\n",
       " 35: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.55755572627892225,\n",
       "  'test_time': 0.3447694778442383,\n",
       "  'train_time': 331.56811594963074},\n",
       " 36: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.6379461924004024,\n",
       "  'test_time': 0.6986348628997803,\n",
       "  'train_time': 739.821534872055},\n",
       " 37: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.55783158051095327,\n",
       "  'test_time': 0.9284627437591553,\n",
       "  'train_time': 579.0224933624268},\n",
       " 38: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.57684789700398742,\n",
       "  'test_time': 0.8676085472106934,\n",
       "  'train_time': 1039.9165687561035},\n",
       " 39: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.60678683355818153,\n",
       "  'test_time': 0.8085618019104004,\n",
       "  'train_time': 850.8696830272675},\n",
       " 40: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.67728660264555507,\n",
       "  'test_time': 0.5302472114562988,\n",
       "  'train_time': 684.590704202652},\n",
       " 41: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.57179100313089604,\n",
       "  'test_time': 0.8323392868041992,\n",
       "  'train_time': 953.4868705272675},\n",
       " 42: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.53495167111302333,\n",
       "  'test_time': 0.6918811798095703,\n",
       "  'train_time': 369.9791021347046},\n",
       " 43: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.64167214267408357,\n",
       "  'test_time': 0.5845298767089844,\n",
       "  'train_time': 594.3473381996155},\n",
       " 44: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.64123445596064044,\n",
       "  'test_time': 3.0262045860290527,\n",
       "  'train_time': 645.5469307899475},\n",
       " 45: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.63490949628655136,\n",
       "  'test_time': 1.2711153030395508,\n",
       "  'train_time': 618.851898431778},\n",
       " 46: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.64112315059798719,\n",
       "  'test_time': 1.5398163795471191,\n",
       "  'train_time': 915.682864189148},\n",
       " 47: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.63273683774041301,\n",
       "  'test_time': 1.7117571830749512,\n",
       "  'train_time': 635.817152261734},\n",
       " 48: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.66905749108622947,\n",
       "  'test_time': 1.13885498046875,\n",
       "  'train_time': 1407.011858701706},\n",
       " 49: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.60958611426102738,\n",
       "  'test_time': 1.6158428192138672,\n",
       "  'train_time': 574.1360251903534},\n",
       " 50: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.64602073382833725,\n",
       "  'test_time': 2.350238084793091,\n",
       "  'train_time': 1362.5017971992493},\n",
       " 51: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.59679281841859855,\n",
       "  'test_time': 0.9922428131103516,\n",
       "  'train_time': 514.2479274272919},\n",
       " 52: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.60193831836505007,\n",
       "  'test_time': 0.14989233016967773,\n",
       "  'train_time': 65.38175177574158},\n",
       " 53: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.60419613228388525,\n",
       "  'test_time': 0.8089258670806885,\n",
       "  'train_time': 1074.449039697647},\n",
       " 54: {'learner': DecisionTreeRegressor(criterion='mse', max_depth=50, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             presort=False, random_state=36, splitter='best'),\n",
       "  'score': 0.62373720494530949,\n",
       "  'test_time': 0.4105556011199951,\n",
       "  'train_time': 565.7925493717194}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
